{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ITAI 1371 Module 11: Hyperparameter Tuning & AutoML\n",
        "\n",
        "Welcome to Module 11 of ITAI 1371! In this notebook, we'll dive deep into **hyperparameter tuning** and explore the exciting world of **AutoML** using classic machine learning methods. Our goal is to understand how tuning hyperparameters affects model performance and how automated tools can help us find the best models.\n",
        "\n",
        "We'll work with the **Wine Quality dataset**, transforming it into a binary classification problem (good vs bad wine). Throughout the lab, you'll find detailed explanations, visualizations, and exercises to build your skills.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 1 - Pre-coded Sections with Explanations\n",
        "\n",
        "### 1. Header & Introduction\n",
        "\n",
        "In this section, we'll outline the goals and structure of today's lab."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Goals for this module:\n",
        "- Understand what hyperparameters are and why they matter.\n",
        "- Learn the difference between parameters and hyperparameters.\n",
        "- Explore the bias-variance tradeoff and how hyperparameters influence it.\n",
        "- Use cross-validation to evaluate models properly.\n",
        "- Perform manual hyperparameter tuning and automated methods like Grid Search and Random Search.\n",
        "- Try out AutoML using AutoGluon and compare results.\n",
        "\n",
        "### Dataset:\n",
        "- Wine Quality dataset (binary classification: good vs bad wine).\n",
        "\n",
        "Let's get started!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Deep Dive: Hyperparameters in Different Algorithms\n",
        "\n",
        "Different machine learning algorithms have different hyperparameters. Understanding what each hyperparameter controls is crucial for effective tuning. Let's explore hyperparameters across several common algorithms:\n",
        "\n",
        "**Random Forest:**\n",
        "- `n_estimators`: Number of trees in the forest. More trees generally improve performance but increase computation time.\n",
        "- `max_depth`: Maximum depth of each tree. Controls model complexity and overfitting.\n",
        "- `min_samples_split`: Minimum samples required to split a node. Higher values prevent overfitting.\n",
        "- `max_features`: Number of features to consider for each split. Adds randomness to reduce overfitting.\n",
        "\n",
        "**Support Vector Machine (SVM):**\n",
        "- `C`: Regularization parameter. Lower values mean stronger regularization.\n",
        "- `kernel`: Type of kernel function (linear, rbf, poly). Determines decision boundary shape.\n",
        "- `gamma`: Kernel coefficient for RBF. Controls influence of single training examples.\n",
        "\n",
        "**Logistic Regression:**\n",
        "- `C`: Inverse regularization strength. Smaller values mean stronger regularization.\n",
        "- `penalty`: Type of regularization (l1, l2, elasticnet).\n",
        "- `solver`: Algorithm for optimization (lbfgs, saga, etc.)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Viewing default hyperparameters\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Create models with default hyperparameters\n",
        "rf = RandomForestClassifier()\n",
        "svm = SVC()\n",
        "lr = LogisticRegression()\n",
        "\n",
        "print(\"Random Forest Default Hyperparameters:\")\n",
        "print(f\"  n_estimators: {rf.n_estimators}\")\n",
        "print(f\"  max_depth: {rf.max_depth}\")\n",
        "print(f\"  min_samples_split: {rf.min_samples_split}\")\n",
        "print(f\"  max_features: {rf.max_features}\")\n",
        "\n",
        "print(\"\\nSVM Default Hyperparameters:\")\n",
        "print(f\"  C: {svm.C}\")\n",
        "print(f\"  kernel: {svm.kernel}\")\n",
        "print(f\"  gamma: {svm.gamma}\")\n",
        "\n",
        "print(\"\\nLogistic Regression Default Hyperparameters:\")\n",
        "print(f\"  C: {lr.C}\")\n",
        "print(f\"  penalty: {lr.penalty}\")\n",
        "print(f\"  solver: {lr.solver}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### \ud83e\udd14 Knowledge Check: Hyperparameter Understanding\n",
        "\n",
        "**Question 1 (Conceptual):** If you wanted to reduce overfitting in a Random Forest model, which hyperparameters would you adjust and in what direction?\n",
        "\n",
        "*Your answer:* [Write your answer here]\n",
        "\n",
        "---\n",
        "\n",
        "**Question 2 (Reflective):** \ud83d\udcad Why do you think scikit-learn chose these specific default values? What assumptions might they be making about typical datasets?\n",
        "\n",
        "*Your answer:* [Write your answer here]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Conceptual Question:\n",
        "- What do you think hyperparameters are and why might tuning them be important for machine learning models?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Deep Dive: Hyperparameters in Different Algorithms\n",
        "\n",
        "Different machine learning algorithms have different hyperparameters. Understanding what each hyperparameter controls is crucial for effective tuning. Let's explore hyperparameters across several common algorithms:\n",
        "\n",
        "**Random Forest:**\n",
        "- `n_estimators`: Number of trees in the forest. More trees generally improve performance but increase computation time.\n",
        "- `max_depth`: Maximum depth of each tree. Controls model complexity and overfitting.\n",
        "- `min_samples_split`: Minimum samples required to split a node. Higher values prevent overfitting.\n",
        "- `max_features`: Number of features to consider for each split. Adds randomness to reduce overfitting.\n",
        "\n",
        "**Support Vector Machine (SVM):**\n",
        "- `C`: Regularization parameter. Lower values mean stronger regularization.\n",
        "- `kernel`: Type of kernel function (linear, rbf, poly). Determines decision boundary shape.\n",
        "- `gamma`: Kernel coefficient for RBF. Controls influence of single training examples.\n",
        "\n",
        "**Logistic Regression:**\n",
        "- `C`: Inverse regularization strength. Smaller values mean stronger regularization.\n",
        "- `penalty`: Type of regularization (l1, l2, elasticnet).\n",
        "- `solver`: Algorithm for optimization (lbfgs, saga, etc.)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Viewing default hyperparameters\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Create models with default hyperparameters\n",
        "rf = RandomForestClassifier()\n",
        "svm = SVC()\n",
        "lr = LogisticRegression()\n",
        "\n",
        "print(\"Random Forest Default Hyperparameters:\")\n",
        "print(f\"  n_estimators: {rf.n_estimators}\")\n",
        "print(f\"  max_depth: {rf.max_depth}\")\n",
        "print(f\"  min_samples_split: {rf.min_samples_split}\")\n",
        "print(f\"  max_features: {rf.max_features}\")\n",
        "\n",
        "print(\"\\nSVM Default Hyperparameters:\")\n",
        "print(f\"  C: {svm.C}\")\n",
        "print(f\"  kernel: {svm.kernel}\")\n",
        "print(f\"  gamma: {svm.gamma}\")\n",
        "\n",
        "print(\"\\nLogistic Regression Default Hyperparameters:\")\n",
        "print(f\"  C: {lr.C}\")\n",
        "print(f\"  penalty: {lr.penalty}\")\n",
        "print(f\"  solver: {lr.solver}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### \ud83e\udd14 Knowledge Check: Hyperparameter Understanding\n",
        "\n",
        "**Question 1 (Conceptual):** If you wanted to reduce overfitting in a Random Forest model, which hyperparameters would you adjust and in what direction?\n",
        "\n",
        "*Your answer:* [Write your answer here]\n",
        "\n",
        "---\n",
        "\n",
        "**Question 2 (Reflective):** \ud83d\udcad Why do you think scikit-learn chose these specific default values? What assumptions might they be making about typical datasets?\n",
        "\n",
        "*Your answer:* [Write your answer here]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "### 2. Setup & Data Loading\n",
        "\n",
        "Let's load the Wine Quality dataset and prepare it for binary classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the Wine Quality dataset\n",
        "# Dataset link: https://archive.ics.uci.edu/ml/datasets/Wine+Quality\n",
        "# We'll use the red wine dataset\n",
        "\n",
        "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv'\n",
        "wine_df = pd.read_csv(url, sep=';')\n",
        "wine_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dataset Description:\n",
        "- 1599 samples\n",
        "- 11 physicochemical features (e.g., acidity, sugar, pH)\n",
        "- Target: quality score (0-10 scale)\n",
        "\n",
        "Our goal: convert quality into a **binary classification** problem:\n",
        "- Quality >= 7 -> Good wine (label 1)\n",
        "- Quality < 7 -> Bad wine (label 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create binary target\n",
        "wine_df['good_quality'] = (wine_df['quality'] >= 7).astype(int)\n",
        "\n",
        "# Drop original quality column\n",
        "wine_df = wine_df.drop('quality', axis=1)\n",
        "\n",
        "# Check distribution\n",
        "wine_df['good_quality'].value_counts(normalize=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The dataset is imbalanced, with fewer good quality wines.\n",
        "\n",
        "### Split dataset into train and test sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "X = wine_df.drop('good_quality', axis=1)\n",
        "y = wine_df['good_quality']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"Training samples: {X_train.shape[0]}\")\n",
        "print(f\"Testing samples: {X_test.shape[0]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Feature Scaling\n",
        "\n",
        "Many ML algorithms perform better with scaled features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "### 3. Understanding Hyperparameters (with examples)\n",
        "\n",
        "Let's clarify what hyperparameters are by looking at two classic models: Logistic Regression and Random Forest."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Deep Dive: Hyperparameters in Different Algorithms\n",
        "\n",
        "Different machine learning algorithms have different hyperparameters. Understanding what each hyperparameter controls is crucial for effective tuning. Let's explore hyperparameters across several common algorithms:\n",
        "\n",
        "**Random Forest:**\n",
        "- `n_estimators`: Number of trees in the forest. More trees generally improve performance but increase computation time.\n",
        "- `max_depth`: Maximum depth of each tree. Controls model complexity and overfitting.\n",
        "- `min_samples_split`: Minimum samples required to split a node. Higher values prevent overfitting.\n",
        "- `max_features`: Number of features to consider for each split. Adds randomness to reduce overfitting.\n",
        "\n",
        "**Support Vector Machine (SVM):**\n",
        "- `C`: Regularization parameter. Lower values mean stronger regularization.\n",
        "- `kernel`: Type of kernel function (linear, rbf, poly). Determines decision boundary shape.\n",
        "- `gamma`: Kernel coefficient for RBF. Controls influence of single training examples.\n",
        "\n",
        "**Logistic Regression:**\n",
        "- `C`: Inverse regularization strength. Smaller values mean stronger regularization.\n",
        "- `penalty`: Type of regularization (l1, l2, elasticnet).\n",
        "- `solver`: Algorithm for optimization (lbfgs, saga, etc.)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Viewing default hyperparameters\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Create models with default hyperparameters\n",
        "rf = RandomForestClassifier()\n",
        "svm = SVC()\n",
        "lr = LogisticRegression()\n",
        "\n",
        "print(\"Random Forest Default Hyperparameters:\")\n",
        "print(f\"  n_estimators: {rf.n_estimators}\")\n",
        "print(f\"  max_depth: {rf.max_depth}\")\n",
        "print(f\"  min_samples_split: {rf.min_samples_split}\")\n",
        "print(f\"  max_features: {rf.max_features}\")\n",
        "\n",
        "print(\"\\nSVM Default Hyperparameters:\")\n",
        "print(f\"  C: {svm.C}\")\n",
        "print(f\"  kernel: {svm.kernel}\")\n",
        "print(f\"  gamma: {svm.gamma}\")\n",
        "\n",
        "print(\"\\nLogistic Regression Default Hyperparameters:\")\n",
        "print(f\"  C: {lr.C}\")\n",
        "print(f\"  penalty: {lr.penalty}\")\n",
        "print(f\"  solver: {lr.solver}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### \ud83e\udd14 Knowledge Check: Hyperparameter Understanding\n",
        "\n",
        "**Question 1 (Conceptual):** If you wanted to reduce overfitting in a Random Forest model, which hyperparameters would you adjust and in what direction?\n",
        "\n",
        "*Your answer:* [Write your answer here]\n",
        "\n",
        "---\n",
        "\n",
        "**Question 2 (Reflective):** \ud83d\udcad Why do you think scikit-learn chose these specific default values? What assumptions might they be making about typical datasets?\n",
        "\n",
        "*Your answer:* [Write your answer here]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Logistic Regression hyperparameters:**\n",
        "- `C`: inverse of regularization strength\n",
        "- `max_iter`: max number of iterations\n",
        "\n",
        "**Random Forest hyperparameters:**\n",
        "- `n_estimators`: number of trees\n",
        "- `max_depth`: max depth of trees\n",
        "- `max_features`: number of features considered for best split\n",
        "\n",
        "Let's see their effect on model performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Logistic Regression with different C values\n",
        "C_values = [0.01, 0.1, 1, 10, 100]\n",
        "logreg_scores = []\n",
        "for c in C_values:\n",
        "    model = LogisticRegression(C=c, max_iter=200, random_state=42)\n",
        "    model.fit(X_train_scaled, y_train)\n",
        "    preds = model.predict(X_test_scaled)\n",
        "    acc = accuracy_score(y_test, preds)\n",
        "    logreg_scores.append(acc)\n",
        "\n",
        "# Plot results\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(C_values, logreg_scores, marker='o')\n",
        "plt.xscale('log')\n",
        "plt.xlabel('C (Inverse Regularization Strength)')\n",
        "plt.ylabel('Test Accuracy')\n",
        "plt.title('Effect of C on Logistic Regression Performance')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Interpretation:\n",
        "- Smaller `C` means stronger regularization (simpler model).\n",
        "- Larger `C` means less regularization (more complex model).\n",
        "- Accuracy changes with these hyperparameters, demonstrating their importance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Random Forest with different max_depth values\n",
        "depth_values = [2, 4, 6, 8, 10, None]\n",
        "rf_scores = []\n",
        "for depth in depth_values:\n",
        "    model = RandomForestClassifier(n_estimators=100, max_depth=depth, random_state=42)\n",
        "    model.fit(X_train, y_train)  # no scaling needed for RF\n",
        "    preds = model.predict(X_test)\n",
        "    acc = accuracy_score(y_test, preds)\n",
        "    rf_scores.append(acc)\n",
        "\n",
        "# Plot results\n",
        "plt.figure(figsize=(8,5))\n",
        "labels = ['2', '4', '6', '8', '10', 'None']\n",
        "plt.plot(labels, rf_scores, marker='o')\n",
        "plt.xlabel('max_depth')\n",
        "plt.ylabel('Test Accuracy')\n",
        "plt.title('Effect of max_depth on Random Forest Performance')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Conceptual Question:\n",
        "- How might changing the `max_depth` affect underfitting or overfitting? Why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "### 4. Parameters vs Hyperparameters (detailed explanation)\n",
        "\n",
        "**Parameters:**\n",
        "- Learned from the data during training.\n",
        "- Example: Coefficients in logistic regression, split thresholds in decision trees.\n",
        "\n",
        "**Hyperparameters:**\n",
        "- Set before training.\n",
        "- Control the training process and model complexity.\n",
        "- Examples: learning rate, number of trees, max depth.\n",
        "\n",
        "Understanding the difference helps us tune models effectively."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Reflective Question:\n",
        "- Why can't we learn hyperparameters directly from the training data like parameters?\n",
        "- How might hyperparameters impact the final model's ability to generalize?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "### 5. Bias-Variance Tradeoff (with visualizations)\n",
        "\n",
        "The bias-variance tradeoff is a fundamental concept:\n",
        "- **Bias:** Error from erroneous assumptions in the model.\n",
        "- **Variance:** Error from sensitivity to small fluctuations in the training set.\n",
        "\n",
        "Hyperparameters often control this tradeoff."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualizing bias-variance tradeoff with polynomial regression on synthetic data\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "np.random.seed(42)\n",
        "X_synthetic = np.sort(np.random.rand(40))\n",
        "y_synthetic = np.sin(2 * np.pi * X_synthetic) + np.random.randn(40) * 0.1\n",
        "\n",
        "X_plot = np.linspace(0, 1, 100)\n",
        "\n",
        "degrees = [1, 3, 9]\n",
        "plt.figure(figsize=(14, 4))\n",
        "for i, degree in enumerate(degrees, 1):\n",
        "    model = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n",
        "    model.fit(X_synthetic[:, np.newaxis], y_synthetic)\n",
        "    y_plot = model.predict(X_plot[:, np.newaxis])\n",
        "    \n",
        "    plt.subplot(1, 3, i)\n",
        "    plt.scatter(X_synthetic, y_synthetic, color='black', label='Data')\n",
        "    plt.plot(X_plot, y_plot, label=f'Degree {degree}')\n",
        "    plt.ylim(-2, 2)\n",
        "    plt.legend()\n",
        "    plt.title(f'Polynomial Degree = {degree}')\n",
        "    plt.xlabel('X')\n",
        "    plt.ylabel('y')\n",
        "\n",
        "plt.suptitle('Bias-Variance Tradeoff Visualization')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Explanation:\n",
        "- Degree 1: High bias, underfitting (too simple).\n",
        "- Degree 9: High variance, overfitting (too complex).\n",
        "- Degree 3: Good balance.\n",
        "\n",
        "Hyperparameters like model complexity control this tradeoff."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "### 6. Cross-Validation (demonstration)\n",
        "\n",
        "Cross-validation helps us estimate model performance reliably by splitting the data multiple times."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "model = RandomForestClassifier(n_estimators=100, max_depth=6, random_state=42)\n",
        "scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')\n",
        "\n",
        "print(f\"Cross-validation accuracies: {scores}\")\n",
        "print(f\"Mean CV accuracy: {scores.mean():.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Conceptual Question:\n",
        "- Why is cross-validation considered a better performance estimator than a single train-test split?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "### 7. Manual Hyperparameter Tuning (complete example)\n",
        "\n",
        "Let's manually tune hyperparameters for a Random Forest and see how it affects performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "max_depth_values = [2, 4, 6, 8, 10]\n",
        "n_estimators_values = [10, 50, 100, 200]\n",
        "\n",
        "results = []\n",
        "for depth in max_depth_values:\n",
        "    for n_est in n_estimators_values:\n",
        "        model = RandomForestClassifier(max_depth=depth, n_estimators=n_est, random_state=42)\n",
        "        model.fit(X_train, y_train)\n",
        "        preds = model.predict(X_test)\n",
        "        acc = accuracy_score(y_test, preds)\n",
        "        results.append({'max_depth': depth, 'n_estimators': n_est, 'accuracy': acc})\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize results as heatmap\n",
        "pivot_table = results_df.pivot(index='max_depth', columns='n_estimators', values='accuracy')\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.heatmap(pivot_table, annot=True, fmt='.3f', cmap='YlGnBu')\n",
        "plt.title('Random Forest Accuracy for Different Hyperparameters')\n",
        "plt.ylabel('max_depth')\n",
        "plt.xlabel('n_estimators')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Reflective Question:\n",
        "- Which hyperparameter seemed to have a bigger impact on accuracy? Why might that be?\n",
        "- How would you decide which hyperparameters to tune next based on these results?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "### 8. Grid Search (complete implementation)\n",
        "\n",
        "Grid Search automates the manual hyperparameter tuning by exhaustively searching over a parameter grid."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {\n",
        "    'max_depth': [4, 6, 8],\n",
        "    'n_estimators': [50, 100, 150],\n",
        "    'max_features': ['auto', 'sqrt']\n",
        "}\n",
        "\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "print(f\"Best parameters found: {grid_search.best_params_}\")\n",
        "print(f\"Best cross-validation accuracy: {grid_search.best_score_:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate best model on test set\n",
        "best_rf = grid_search.best_estimator_\n",
        "test_preds = best_rf.predict(X_test)\n",
        "print(\"Test Accuracy:\", accuracy_score(y_test, test_preds))\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, test_preds))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Conceptual Question:\n",
        "- What are some advantages and disadvantages of Grid Search?\n",
        "- How does it compare to manual tuning?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Part 2 - Student Coding Exercises\n",
        "\n",
        "### 9. Random Search (student implements with tips)\n",
        "\n",
        "Unlike Grid Search, Random Search samples hyperparameter combinations randomly.\n",
        "\n",
        "**Exercise:** Implement RandomizedSearchCV on the Random Forest model to find good hyperparameters.\n",
        "\n",
        "Hints:\n",
        "- Use `RandomizedSearchCV` from `sklearn.model_selection`\n",
        "- Define parameter distributions for `max_depth`, `n_estimators`, and `max_features`\n",
        "- Use 20 iterations (`n_iter=20`)\n",
        "- Use 5-fold cross-validation\n",
        "- Evaluate and print best params and accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from scipy.stats import randint\n",
        "\n",
        "# YOUR CODE BELOW\n",
        "\n",
        "# Define parameter distributions\n",
        "param_dist = {\n",
        "    'max_depth': randint(2, 15),\n",
        "    'n_estimators': randint(10, 200),\n",
        "    'max_features': ['auto', 'sqrt', 'log2', None]\n",
        "}\n",
        "\n",
        "# Initialize model\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Initialize RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=rf,\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=20,\n",
        "    cv=5,\n",
        "    scoring='accuracy',\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Fit to training data\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Print best parameters and accuracy\n",
        "print(f\"Best parameters found: {random_search.best_params_}\")\n",
        "print(f\"Best cross-validation accuracy: {random_search.best_score_:.4f}\")\n",
        "\n",
        "# Evaluate on test set\n",
        "best_rf_random = random_search.best_estimator_\n",
        "test_preds_random = best_rf_random.predict(X_test)\n",
        "print(f\"Test Accuracy: {accuracy_score(y_test, test_preds_random):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Reflective Question:\n",
        "- How did the results from Random Search compare to Grid Search?\n",
        "- Why might Random Search be more efficient than Grid Search in some cases?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "### 10. Comparing Methods (student analysis)\n",
        "\n",
        "**Exercise:**\n",
        "- Compare the performances of manual tuning, grid search, and random search.\n",
        "- Create a summary DataFrame with method, best hyperparameters, and test accuracy.\n",
        "- Discuss advantages and disadvantages of each approach."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# YOUR CODE BELOW\n",
        "\n",
        "summary = pd.DataFrame({\n",
        "    'Method': ['Manual Tuning', 'Grid Search', 'Random Search'],\n",
        "    'Best Hyperparameters': [\n",
        "        results_df.loc[results_df['accuracy'].idxmax(), ['max_depth', 'n_estimators']].to_dict(),\n",
        "        grid_search.best_params_,\n",
        "        random_search.best_params_\n",
        "    ],\n",
        "    'Test Accuracy': [\n",
        "        results_df['accuracy'].max(),\n",
        "        accuracy_score(y_test, best_rf.predict(X_test)),\n",
        "        accuracy_score(y_test, best_rf_random.predict(X_test))\n",
        "    ]\n",
        "})\n",
        "\n",
        "summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Reflective Question:\n",
        "- Which method would you choose for a real-world project and why?\n",
        "- What factors (time, computational resources, dataset size) might influence your choice?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "### 11. AutoML with AutoGluon (guided then student exercise)\n",
        "\n",
        "AutoML tools like AutoGluon automate the entire model building and tuning process.\n",
        "\n",
        "Let's first install and import AutoGluon, then run an AutoML experiment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Uncomment the next line to install AutoGluon if it's not installed\n",
        "# !pip install autogluon.tabular -q\n",
        "\n",
        "from autogluon.tabular import TabularPredictor\n",
        "\n",
        "# Prepare DataFrame for AutoGluon\n",
        "train_data = X_train.copy()\n",
        "train_data['good_quality'] = y_train.values\n",
        "\n",
        "test_data = X_test.copy()\n",
        "test_data['good_quality'] = y_test.values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Guided Exercise:**\n",
        "Train an AutoGluon predictor and evaluate performance.\n",
        "\n",
        "Use `time_limit=60` seconds for quick demonstration.\n",
        "\n",
        "Observe the leaderboard and test accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# YOUR CODE BELOW\n",
        "\n",
        "predictor = TabularPredictor(label='good_quality', eval_metric='accuracy').fit(\n",
        "    train_data=train_data,\n",
        "    time_limit=60,\n",
        "    verbosity=2\n",
        ")\n",
        "\n",
        "# Show leaderboard\n",
        "leaderboard = predictor.leaderboard(silent=True)\n",
        "leaderboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate on test data\n",
        "y_pred_automl = predictor.predict(test_data.drop(columns=['good_quality']))\n",
        "acc_automl = accuracy_score(test_data['good_quality'], y_pred_automl)\n",
        "print(f\"AutoGluon Test Accuracy: {acc_automl:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Student Exercise:\n",
        "- Try increasing the `time_limit` to 180 seconds and retrain.\n",
        "- Observe how the leaderboard and accuracy change.\n",
        "- Experiment with excluding some model types by setting `excluded_model_types` in `fit()`.\n",
        "- Report your findings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "### 12. Final Comparison & Reflection\n",
        "\n",
        "Summarize your findings across all tuning methods:\n",
        "- Manual tuning\n",
        "- Grid Search\n",
        "- Random Search\n",
        "- AutoML (AutoGluon)\n",
        "\n",
        "Consider accuracy, time, complexity, and ease of use."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Reflective Questions:\n",
        "- What tradeoffs did you observe between manual and automated tuning?\n",
        "- When might AutoML be preferred? When might manual tuning still be useful?\n",
        "- How does understanding hyperparameters help even when using AutoML?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use this cell to write your summary and answers\n",
        "from IPython.display import display\n",
        "\n",
        "summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "### Congratulations! You have completed Module 11 on Hyperparameter Tuning & AutoML.\n",
        "\n",
        "Please submit your notebook and reflect on how hyperparameter tuning can be incorporated into your future machine learning projects."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}